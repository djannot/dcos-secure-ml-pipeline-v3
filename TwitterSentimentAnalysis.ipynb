{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.local/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = spark.read.json(\"hdfs:///user/nobody/tweet-lake/raw/2018/08/14/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive_tweets = spark.sql(\"SELECT text FROM tweets WHERE (text LIKE '%\\U0001F60D%' OR text LIKE '%\\U0001F60A%' OR text LIKE '%\\U0001F604%' OR text LIKE '%\\U0001F603%' OR text LIKE '%\\U0001F600%' OR text LIKE '%\\U0001F606%') AND text NOT LIKE '%\\U0001F62D%' AND text NOT LIKE '%\\U0001F612%' AND text NOT LIKE '%\\U0001F629%' AND text NOT LIKE '%\\U0001F61E%' AND text NOT LIKE '%\\U0001F62A%'\")\n",
    "df_positive_tweets.createOrReplaceTempView(\"positive_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative_tweets = spark.sql(\"SELECT text FROM tweets WHERE (text LIKE '%\\U0001F62D%' OR text LIKE '%\\U0001F612%' OR text LIKE '%\\U0001F629%' OR text LIKE '%\\U0001F61E%' OR text LIKE '%\\U0001F62A%') AND text NOT LIKE '%\\U0001F60D%' AND text NOT LIKE '%\\U0001F60A%' AND text NOT LIKE '%\\U0001F604%' AND text NOT LIKE '%\\U0001F603%' AND text NOT LIKE '%\\U0001F600%' AND text NOT LIKE '%\\U0001F606%'\")\n",
    "df_negative_tweets.createOrReplaceTempView(\"negative_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126086"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_tweets = spark.sql(\"SELECT text FROM tweets WHERE text NOT LIKE '%\\U0001F60D%' AND text NOT LIKE '%\\U0001F60A%' AND text NOT LIKE '%\\U0001F604%' AND text NOT LIKE '%\\U0001F603%' AND text NOT LIKE '%\\U0001F600%' AND text NOT LIKE '%\\U0001F606%' AND text NOT LIKE '%\\U0001F60D%' AND text NOT LIKE '%\\U0001F60A%' AND text NOT LIKE '%\\U0001F604%' AND text NOT LIKE '%\\U0001F603%' AND text NOT LIKE '%\\U0001F600%' AND text NOT LIKE '%\\U0001F606%'\")\n",
    "other_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3129"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiments = spark.sql(\"(SELECT text, CAST(1 AS DOUBLE) AS sentiment FROM positive_tweets) UNION ALL (SELECT text, CAST(0 AS DOUBLE) AS sentiment FROM negative_tweets) ORDER BY RAND()\")\n",
    "df_sentiments.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(row):\n",
    "    text = row.text\n",
    "    stripped = re.sub(combined_pat, '', text)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    simple_spaced = re.sub(' +',' ',letters_only)\n",
    "    return simple_spaced, row.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_clean = df_sentiments.rdd.map(tweet_cleaner_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = rdd_clean.toDF([\"text\",\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|text                                                                                                                      |sentiment|\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "| u u                                                                                                                      |1.0      |\n",
      "| abc                                                                                                                      |0.0      |\n",
      "| mustwatch list week bepannaah udaan ishqbaaz kumkumbhagya                                                                |1.0      |\n",
      "|rt mau lulusan sekolah negeri swasta agama gak masalah tapi gak usa ngaku jadi santri aja belum sudah ngaku post santri m |1.0      |\n",
      "|                                                                                                                          |1.0      |\n",
      "|porque nunca me acord que en las playas hay muchas chivillas                                                              |0.0      |\n",
      "|rt hello bro morning din sayo musta na ingat ok aldubamidstthestorm                                                       |1.0      |\n",
      "|rt                                                                                                                        |1.0      |\n",
      "|rt c est toujours la meilleure interview de l histoire regardez la fin quand il s excuse au pr s du robinet               |0.0      |\n",
      "| mbok tuh kan biasanya buat art bukan sih yaudah kalau gitu kamu dosa sendiri aja aku lebih pilih nika                    |0.0      |\n",
      "|rt                                                                                                                        |0.0      |\n",
      "| mizukimail                                                                                                               |1.0      |\n",
      "|                                                                                                                          |0.0      |\n",
      "|rt                                                                                                                        |0.0      |\n",
      "|rt beauty at its best love you madhuri ma am have a blessed day ahead eternalbeauty                                       |1.0      |\n",
      "|sou muito f amo tu                                                                                                        |1.0      |\n",
      "|rt                                                                                                                        |0.0      |\n",
      "|rt so lyric just posted on instagram that she is pregnant by a after all this drama throws this whole season away         |0.0      |\n",
      "|                                                                                                                          |1.0      |\n",
      "|rt support for u cr                                                                                                       |1.0      |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "| mustwatch list w...|      1.0|\n",
      "|rt mau lulusan se...|      1.0|\n",
      "|porque nunca me a...|      0.0|\n",
      "|rt hello bro morn...|      1.0|\n",
      "|rt c est toujours...|      0.0|\n",
      "| mbok tuh kan bia...|      0.0|\n",
      "|rt beauty at its ...|      1.0|\n",
      "|rt so lyric just ...|      0.0|\n",
      "|rt after three ce...|      0.0|\n",
      "| ate riiii pag na...|      1.0|\n",
      "|rt he s captured ...|      1.0|\n",
      "|rt whitepaper rea...|      1.0|\n",
      "|rt oh deer i will...|      1.0|\n",
      "|rt bila aku ambik...|      0.0|\n",
      "|do al ah ap detay...|      1.0|\n",
      "|rt jimin asked ju...|      1.0|\n",
      "|rt nice g zel uzu...|      1.0|\n",
      "|excited na ko par...|      1.0|\n",
      "|rt my article is ...|      1.0|\n",
      "|ay no yo de verda...|      0.0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.createOrReplaceTempView(\"clean\")\n",
    "df_final = spark.sql(\"SELECT * FROM clean WHERE LENGTH(text) > 50\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df_final.randomSplit([0.98, 0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|sentiment|               words|                  tf|            features|label|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+-----+\n",
      "|                abc |      0.0|             [, abc]|(65536,[52148,525...|(65536,[52148,525...|  1.0|\n",
      "| mbok tuh kan bia...|      0.0|[, mbok, tuh, kan...|(65536,[10158,107...|(65536,[10158,107...|  1.0|\n",
      "|          mizukimail|      1.0|      [, mizukimail]|(65536,[45789,525...|(65536,[45789,525...|  0.0|\n",
      "| mustwatch list w...|      1.0|[, mustwatch, lis...|(65536,[338,3850,...|(65536,[338,3850,...|  0.0|\n",
      "|                u u |      1.0|            [, u, u]|(65536,[15318,525...|(65536,[15318,525...|  0.0|\n",
      "+--------------------+---------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8798076923076924"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save(\"hdfs:///tweets_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
